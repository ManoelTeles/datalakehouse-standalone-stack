version: "3.8"
services:
  # Postgres
  pgairflow:
    build:
      context: ./
      dockerfile: ./postgres/pgairflow.dockerfile
    image: manoelteles/pgairflow:v1
    container_name: pgairflow
    ports:
      - "5432"
    networks:
      - nwDatalakeHouse
    volumes:
      - pgdata:/var/lib/postgresql/data/
  # Airflow
  airflow-webserver:
    build:
      context: ./
      dockerfile: ./airflow/airflow.dockerfile
    image: manoelteles/airflow:v1
    container_name: airflow
    restart: always
    ports:
      - "8080:8080"
    networks:
      - nwDatalakeHouse
    volumes:
      - ../dags:/usr/local/airflow/dags #DAG folder
      - ../spark/app:/usr/local/spark/app #Spark Scripts (Must be the same path in airflow and Spark Cluster)
      - ../spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
    depends_on:
        - pgairflow
    environment:
        - LOAD_EX=n
        - EXECUTOR=Local
        - AIRFLOW__CORE__DAGS_FOLDER=/usr/local/airflow/dags
        - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
        - AIRFLOW__CORE__LOAD_EXAMPLES=False
        # - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=true expose the configs from airflow.cfg
    command: webserver
    healthcheck:
        test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
        interval: 30s
        timeout: 30s
        retries: 3




  # # Spark with 3 workers
  # spark:
  #     image: bitnami/spark:3.3.2
  #     user: root # https://docs.bitnami.com/tutorials/work-with-non-root-containers/
  #     hostname: spark
  #     networks:
  #         - nwDatalakeHouse
  #     environment:
  #         - SPARK_MODE=master
  #         - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #         - SPARK_RPC_ENCRYPTION_ENABLED=no
  #         - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #         - SPARK_SSL_ENABLED=no
  #     volumes:
  #         - ../spark/app:/usr/local/spark/app
  #         - ../spark/resources:/usr/local/spark/resources
  #     ports:
  #         - "8181:8080"
  #         - "7077:7077"

  # spark-worker-1:
  #     image: bitnami/spark:3.3.2
  #     user: root
  #     networks:
  #         - nwDatalakeHouse
  #     environment:
  #         - SPARK_MODE=worker
  #         - SPARK_MASTER_URL=spark://spark:7077
  #         - SPARK_WORKER_MEMORY=1G
  #         - SPARK_WORKER_CORES=1
  #         - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #         - SPARK_RPC_ENCRYPTION_ENABLED=no
  #         - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #         - SPARK_SSL_ENABLED=no
  #     volumes:
  #         - ../spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
  #         - ../spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)

  # spark-worker-2:
  #     image: bitnami/spark:3.3.2
  #     user: root
  #     networks:
  #         - nwDatalakeHouse
  #     environment:
  #         - SPARK_MODE=worker
  #         - SPARK_MASTER_URL=spark://spark:7077
  #         - SPARK_WORKER_MEMORY=1G
  #         - SPARK_WORKER_CORES=1
  #         - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #         - SPARK_RPC_ENCRYPTION_ENABLED=no
  #         - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #         - SPARK_SSL_ENABLED=no
  #     volumes:
  #         - ../spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
  #         - ../spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)

  # spark-worker-3:
  #     image: bitnami/spark:3.3.2
  #     user: root
  #     networks:
  #         - nwDatalakeHouse
  #     environment:
  #         - SPARK_MODE=worker
  #         - SPARK_MASTER_URL=spark://spark:7077
  #         - SPARK_WORKER_MEMORY=1G
  #         - SPARK_WORKER_CORES=1
  #         - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #         - SPARK_RPC_ENCRYPTION_ENABLED=no
  #         - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #         - SPARK_SSL_ENABLED=no
  #     volumes:
  #         - ../spark/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
  #         - ../spark/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)

  # #Jupyter notebook
  # jupyter-spark:
  #     image: jupyter/pyspark-notebook:spark-3.3.2
  #     networks:
  #         - nwDatalakeHouse
  #     ports:
  #       - "8888:8888"
  #       - "4040-4080:4040-4080"
  #     volumes:
  #       - ../notebooks:/home/jovyan/work/notebooks/
  #       - ../spark/resources/data:/home/jovyan/work/data/
  #       - ../spark/resources/jars:/home/jovyan/work/jars/
  #     environment:
  #       JUPYTER_ENABLE_LAB: "yes"
  #     command: "start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''"

networks:
  nwDatalakeHouse:
    driver: bridge

volumes:
  pgdata:
